---
title: "20201102_Pathak_Vinayak_HW3"
author: Vinayak Pathak
date: 2020-11-07
output: html_document
---

# BME 590 Homework 3

---

*In this homework, the objectives are to*

1. Use R to preprocess and examine a biomedical dataset

2. Implement multiple unsupervised learning methods in a real-world biomedical problem, including: Principal Component Analysis, Hierarchical Clustering, and K-means Clustering in R

3. Visualize and understand principal components, hierarchical clustering dendrograms, and the outputs of K-means clustering in R

Assignments will only be accepted in electronic format in RMarkdown (.rmd) files and knitted .html files. You must submit both the RMD file and HTML file for each HW. **5 points will be deducted for every assignment submission that does not include either the RMarkdown file or the knitted html file.** Your code should be adequately commented to clearly explain the steps you used to produce the analyses. RMarkdown homework files should be uploaded to Sakai with the naming convention date_lastname_firstname_HW[X].Rmd. For example, my first homework assignment would be named 20200911_Dunn_Jessilyn_HW1.Rmd. **It is important to note that 5 points will be deducted for every assignment that is named improperly.** Please add your answer to each question directly after the question prompt in the homework .Rmd file template provided below.

```{r message=FALSE}
library(tidyverse)
library(ggplot2)
library(lubridate)
library(patchwork)
library(gridExtra)
library(psych)
library(corrplot)
library(ggfortify)
library(factoextra)
```

## Dataset
Breast Cancer Data
https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)

## Data Preparation
1. Download the cancer data titled "HW3_data.csv" from Sakai and import it into R. Look at the first 10 lines of the data to learn about the dataset. The “diagnosis” field shows whether the patient was diagnosed with a benign or malignant tumor. Please read additional information about each column available in the file "HW3_data_info.html", which can also be downloaded from Sakai.
```{r}
data<-read_csv(file = "HW3_data.csv")
```
2. Answer the following questions by using the summary function or other methods of your choice:

a. How many observations are there?
```{r}
length(data[["id"]])
```
Answer a. There are 569 variables.
b. How many independent variables are there?
```{r}

total_independent_variables = length(names(data))-2 
#Ignoring the ID
print(total_independent_variables)
```
Answer b. Total Independent variable = 31
c. Which variables have missing values and how many values were missing in each?

```{r}
var_name<-names(data)#Variable storing the variables in a data frame
logic_missing=list()
```
```{r}
for(i in var_name)
  if(NA %in% data[[i]])
    logic_missing<-append(logic_missing, i)

print(logic_missing)
  
```
```{r}
print(logic_missing)
```
Answer c. Hence the data is missing in texture_mean and age



d. How many observations are there with malignant diagnosis and how many are there with benign diagnosis?
```{r}
summary(as.factor(data$diagnosis))
```
There are total 357 Benign and 212 Malignant tumour
**For this question, please type your answers clearly outside of R chunks, and do not just show the result of running your codes.**

3. There is one column with a very large number of missing observations. Which one is it? We are going to drop this column because it does not make sense to impute observations in this column- why not?. Briefly explain why we should not impute values in this column. Additionally, change the "id" column into the index column (i.e. turn the ID values into row names) and delete the "id" column. Use str() to display the resulting dataframe.

Answer 3. Age variable has the largest number of NA values. We should not impute the this column because if we carry out complete 
case analysis on this, then it will result in removing out lot of rows. If we do other kind of statistical imputationn such has mean imputation or otherwise, then we don't have enough number of data in the age column to get a distribution from which we can chose.
```{r}
data_modified<-mutate(data, age=NULL)#Removing the age column with large number of NA observations.
```
```{r}
data_modified[["id"]]<-c(1:length(data$id))
```
```{r}
data_modified<-data_modified %>% rename(Index = id)
```



4. After the previous step there are still 5 missing values from a column. Impute these missing values by the method of your choice. Keep in mind that imputation must be performed separately for the two different diagnoses (outcome variable) groups. Briefly explain why we should impute according to the different outcome groups.

First let us change the diagnosis group into factor, so
```{r}
data_modified$diagnosis<-as.factor(data_modified$diagnosis)
```

```{r}
#Applying mean imputation by groups
categories<-unique(data_modified$diagnosis)
```
```{r}
impute_fun<-function(df, x){
  a1<-df %>%filter(diagnosis == x) 
  a1$texture_mean[which(is.na(a1$texture_mean))]<-mean(a1$texture_mean, na.rm= T)
  return(a1)
}
```
```{r}
a4<-lapply(categories, function(x)impute_fun(data_modified, x))

```
```{r}
row_count<-sapply(a4, nrow)
```
```{r}
data_final<-do.call(rbind,a4)
```
```{r}
data_final<-arrange(data_final, Index)
```
5. After imputation, use "ggplot" and "facet_wrap" to plot a grid of 10 x 3 histograms to explore the data shape and distribution of all the independent variables in this dataset. The dataset has 10 sets of independent variables not counting age, and each set consists of the mean, standard error and worst value of a particular cell measurement. "Area" and "concavity" are cell measurements. For example, "area_se" is the standard error of area measurements from a particular patient in this study. When you plot, remember to select a reasonable number of bins and add legends and labels when appropriate. Adjust the size of the plot display so that you can see all the facets clearly when you knit. 

```{r, fig.width = 6, fig.height = 8}
ggplot(gather(data_final[, 3:32]), aes(value))+geom_histogram(bins = 20)+facet_wrap(~key, scale = 'free_x', ncol=3)
  #ggplot(aes(val, fill = what)) + geom_histogram() + facet_wrap(c(3:32))
```

6. If you observe the independent variable distributions closely, groups of variables that start with "area" and "concavity" are consistently strongly skewed to the right. Apply log transform using the same formula we used in the last HW to these 6 variables. 
```{r}
df_final3<-mutate_at(data_final, vars(contains("area")|contains("concavity")), function(x)log(x+1))
```
Gather the above data set for plotting the histogram
```{r, fig.width = 6, fig.height = 8}
ggplot(gather(df_final3[, 3:32]), aes(value))+geom_histogram(bins = 20)+facet_wrap(~key, scale = 'free_x', ncol=3)
```

7. The pre-processed dataset needs to be scaled before performing PCA. Can you give a brief explanation on why that is the case? Standardize the dataset. Use summary() again to show that your dataset has been properly standardized.

## PCA
Answer 7.
The pre-processed dataset need to be scaled before performing PCA. This is so that the PCA projects the original data into mutually independent/orthogonal directions in order to maximise thhe variance in each variable. So a proper scaling results in similar mean values for all the variable(which can be zero also) and hence the variable projection is only dependent on the difference in variance/standard deviation.
It also takes care of differences in units/magnitude of different types of variables.  
```{r}
if(!("robustHD" %in% installed.packages()))
install.packages("robustHD")
library("robustHD")
```



```{r}
for(i in c(3:32))
df_final3[,i]<-standardize(df_final3[, i], centerFun = mean, scaleFun = sd)
```
```{r}
summary(df_final3)
```

8. Calculate principal components using function princomp() and print the summary of the results.
```{r}
result_pca<-princomp(df_final3[, c(3:32)], cor = TRUE)

```
```{r}
result_pca2<-prcomp(df_final3[, c(3:32)])
```
```{r}
summary_pca<-summary(result_pca)
```
```{r}
summary(result_pca)
```
```{r}
summary(result_pca2)
```
```{r}

```

9.  Plot a scree plot using the screeplot() function.
```{r}
screeplot(result_pca)
```
10. Plot the following two plots and use patchwork/gridExtra to position the two plots side by side:
a. proportion of variance explained over the number of principal components
```{r}
car_var<-paste("pc", c(1:30), sep = '')
df_pca<-data.frame(PC_comp = car_var, sdev = result_pca$sdev)
df_pca<-mutate(df_pca,var = (sdev)^2)
```
```{r}
a = sum(df_pca$var)
df_pca<-mutate(df_pca,var_prop = var/a)
```


b. cumulative proportion of variance explained plot over the number of principal components; draw horizontal lines at 88% of variance and 95% variance.
```{r}
a = sum(df_pca$var)
df_pca$cum_var<-cumsum(df_pca$var_prop)
```
```{r}

```
```{r}
p1<-df_pca[c(1:20), ]%>%
  ggplot(aes(x = reorder(PC_comp, -var_prop), y = var_prop))+geom_col()+labs(x = "Principal Components", y = "Proportional Variance", title ="Proportion of Variance(First 20 PC)")+theme(plot.title = element_text(hjust = 0.5))
plot(p1)
```
```{r}
p2<-df_pca[c(1:20), ]%>%
  ggplot(aes(x = 1:20, y = cum_var))+geom_point()+geom_line()+scale_x_discrete(limits=c(1:20), labels=paste("pc", c(1:20), sep=""))+geom_hline(yintercept=0.88, linetype = "dashed", color ="red")+geom_hline(yintercept=0.95, linetype = "dashed", color="green")+labs(x = "Principal Components", y = "Cumalative Variance", title ="Cumulative Variance(First 20 PC)")+theme(plot.title = element_text(hjust = 0.5))
plot(p1/p2)
```

Note: please remember to clearly label your plots with titles, axis labels and legends when appropriate.

11. What proportions of variance are captured from the first, second and third principal components? How many principal components do you need to describe at least 88% and 95% of the variance, respectively?
```{r}
df_pca$var_prop[c(1:3)]*100
```
Answer: 44.7%, 19.2%, 9.3% proportion of variance captured from the first, second and third components. 6 principal components are needed to capture 88% variance and 10 principal components are needed to capture 95% of variance.


12. Which are the top 3 variables that contribute the most to the variance captured from PC1, PC2 and PC3 respectively? (hint: look at the loadings information)
```{r}
pc_final_1<-data.frame(load = result_pca$loadings[,1])%>%arrange(desc(load))
head(pc_final_1)
```
concave points_mean , concavity_mean, concave points_worst contribute most to the variance captured from PC1, PC2 and PC3.
14. Because of the relatively large number of variables in this dataset, it's very difficult to see the biplot clearly. Use the autoplot() function in package "ggfortify" to display a clearer biplot overlaid with a scatter plot for the first 2 principal components.
```{r}
biplot(result_pca, choices = 1:2)
```
```{r}
autoplot(result_pca, data =df_final3, color =diagnosis)
```
15. Plot a grid of 3 x 2 scatter plots, where each plot is a scatter plot between two of the first 4 principal components, with different colors for each diagnosis group. For example, in grid cell (1,1), you should plot a scatter plot where the x-axis is PC1 and y-axis is PC2 where observations are colored according to the diagnosis group. Remember to adjust the plot display size so that you can see it clearly. Add legends and labels when appropriate. What are your observations?
```{r}
```

## Hierarchical Clustering

17. Calculate a dissimilarity matrix using Euclidean distance and compute hierarchical clustering using the complete linkage method and plot the dendrogram. Use the rect.hclust() function to display dividing the dendrogram into 4 branches. 
```{r}
library("cluster")
df.dissimilarity<-daisy(df_final3[, c(3:32)], metric ="euclidean")
```

```{r}
df.cluster<-hclust(df.dissimilarity)
```
```{r}
plot(df.cluster)
```

18. Divide the dendrogram into 4 clusters using cutree() function. Then use table() function and the diagnosis label information to compare the composition (benign vs. malignant) of each of the 4 clusters. How would you label each of these four clusters (e.g. cluster 1 is benign or malignant, cluster 2 is …, etc.)?
```{r}
df.cluster.4<-cutree(df.cluster,k = 4, h=1)
```
```{r}
table(df.cluster.4)
```

19. Now try 6 clusters with and plot dendrograms for hierarchical clustering using Ward’s linkage. Use table() function to view the clustering result. How would you label each of these 6 clusters? Does this clustering work better or worse than the clustering result in the previous question? Give brief explanations.
```{r}
df.cluster.6<-cutree(df.cluster,k = 6)
table(df.cluster.6)
plot(df.cluster.6)
```

## K-Means Clustering

20. Compute k-means clustering on this dataset using the kmeans() function for two clusters. Then use the table() function and the diagnosis label information to compare the composition (benign vs. malignant) of each of the 2 clusters (hint: the cluster information from k-means is stored in the $cluster attribute in the k-means result.)
```{r}
kc<-kmeans(df_final3[, 3:32], 2)
```
```{r}

```
```{r}
df_kmeans_plot<-data.frame(kcplot = kc$cluster, diagplot = df_final3$diagnosis)
```
```{r}
table(df_kmeans_plot)
```
```{r}

pkmeans<-ggplot(df_kmeans_plot, aes(x = kcplot, y = diagplot))+geom_point(colour = as.numeric(df_kmeans_plot$kcplot), shape = as.numeric(df_kmeans_plot$diagplot))

plot(pkmeans)
#plot(kc$cluster~df_final3$diagnosis, col = as.numeric(kc$cluster), pch =as.numeric(df_final3$diagnosis))
```
21. Visualize the clusters using the fviz_cluster() function from the factoextra package.
```{r}

library("factoextra")
fviz_cluster(kc, data = df_final3[, 3:32])
```

22. Between the 3 clustering models we have tried in this homework, which one does the best? What's your observation?

